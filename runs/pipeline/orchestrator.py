#!/usr/bin/env python3
"""
Main Pipeline Orchestrator for multi-turn red-teaming.

Implements the extended GALA architecture with:
- GALA learning (tactic-wise and prompt-wise)
- Traceback mechanism
- Tactic generalization

Usage:
    python orchestrator.py --config_path ../../scripts/configs/pipeline_config.yaml
"""

import sys
import os
import json
import logging
from argparse import ArgumentParser
from typing import Dict, Any, List, Optional

from omegaconf import OmegaConf

# Add jailbreak module to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from jailbreak.agentic_module.utils import load_jailbreak_dataset
from jailbreak.agentic_module.agents.attacker import AttackerAgent
from jailbreak.agentic_module.agents.target import TargetAgent
from jailbreak.agentic_module.agents.judge import JudgeAgent
from jailbreak.agentic_module.agents.reasoning_agent import ReasoningAgent
from jailbreak.agentic_module.agents.gala_learning import GALALearningModule
from jailbreak.agentic_module.conversation import ConversationManager

# Add runs/pipeline to path for local imports
sys.path.append(os.path.dirname(__file__))

from orchestrator_utils import (
    TrialResult, GoalResult, ResultStorage,
    format_conversation_for_reasoning, get_config_value
)
from traceback_module import TracebackModule, create_traceback_module
from tactic_generalization import (
    TacticGeneralizationModule, 
    create_tactic_generalization_module,
    save_generalization_result
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PipelineOrchestrator:
    """
    Main orchestrator for the multi-turn red-teaming pipeline.
    
    Coordinates all agents and modules based on config feature flags.
    """
    
    def __init__(self, config):
        """
        Initialize the orchestrator with configuration.
        
        Args:
            config: OmegaConf configuration object
        """
        self.config = config
        
        # Feature flags
        self.enable_gala = get_config_value(config, 'features', 'enable_gala_learning', default=True)
        self.enable_traceback = get_config_value(config, 'features', 'enable_traceback', default=True)
        self.enable_generalization = get_config_value(config, 'features', 'enable_tactic_generalization', default=True)
        self.enable_belief_llm = get_config_value(config, 'features', 'enable_belief_state_llm_update', default=True)
        
        # Pipeline settings
        self.max_turns = get_config_value(config, 'pipeline', 'max_turns', default=5)
        self.max_trials = get_config_value(config, 'pipeline', 'max_trials', default=2)
        
        # Initialize agents
        self._initialize_agents()
        
        # Initialize modules based on feature flags
        self._initialize_modules()
        
        # Initialize storage
        self._initialize_storage()
        
        logger.info(f"PipelineOrchestrator initialized")
        logger.info(f"  Features: GALA={self.enable_gala}, Traceback={self.enable_traceback}, "
                   f"Generalization={self.enable_generalization}")
    
    def _initialize_agents(self) -> None:
        """Initialize all agents from config."""
        logger.info("Initializing agents...")
        
        # Convert OmegaConf to dict for agent initialization
        attacker_config = dict(self.config.agents.attacker)
        reasoning_config = dict(self.config.agents.reasoning)
        target_config = dict(self.config.agents.target)
        judge_config = dict(self.config.agents.judge)
        
        # Attacker agent
        self.attacker = AttackerAgent(
            attacker_config=attacker_config,
            reasoning_config=reasoning_config,
            enable_gala=self.enable_gala
        )
        logger.info("AttackerAgent initialized")
        
        # Target agent
        self.target = TargetAgent(target_config=target_config)
        logger.info("TargetAgent initialized")
        
        # Judge agent
        self.judge = JudgeAgent(judge_config=judge_config)
        logger.info("JudgeAgent initialized")
        
        # Reasoning agent (for belief updates, traceback, generalization)
        self.reasoning = ReasoningAgent(reasoning_config=reasoning_config)
        logger.info("ReasoningAgent initialized")
    
    def _initialize_modules(self) -> None:
        """Initialize optional modules based on feature flags."""
        # GALA learning module
        if self.enable_gala:
            gala_config = {
                'enable_tactic_learning': get_config_value(
                    self.config, 'features', 'enable_tactic_wise_learning', default=True),
                'enable_prompt_learning': get_config_value(
                    self.config, 'features', 'enable_prompt_wise_learning', default=True),
                'enable_knowledge_evolution': get_config_value(
                    self.config, 'features', 'enable_knowledge_evolution', default=True),
            }
            self.gala_module = GALALearningModule(self.attacker, gala_config)
            logger.info("GALALearningModule initialized")
        else:
            self.gala_module = None
        
        # Traceback module
        if self.enable_traceback:
            self.traceback_module = create_traceback_module(self.reasoning, dict(self.config))
            logger.info("TracebackModule initialized")
        else:
            self.traceback_module = None
        
        # Tactic generalization module
        if self.enable_generalization:
            self.generalization_module = create_tactic_generalization_module(
                self.reasoning, dict(self.config))
            logger.info("TacticGeneralizationModule initialized")
        else:
            self.generalization_module = None
    
    def _initialize_storage(self) -> None:
        """Initialize result storage."""
        results_path = get_config_value(self.config, 'output', 'results_path', 
                                        default='pipeline_results.json')
        knowledge_path = get_config_value(self.config, 'output', 'knowledge_path',
                                          default='knowledge_evolution.json')
        successes_path = get_config_value(self.config, 'output', 'successful_jailbreaks_path',
                                          default='successful_jailbreaks.json')
        
        self.storage = ResultStorage(results_path, knowledge_path, successes_path)
    
    def run(self) -> None:
        """Run the complete pipeline."""
        logger.info("üöÄ Starting pipeline")
        
        # Load goals
        dataset = load_jailbreak_dataset(self.config.data)
        
        # Handle DatasetDict with splits (benign/harmful)
        if hasattr(dataset, 'keys') and 'harmful' in dataset.keys():
            harmful_data = dataset['harmful']
            goals = [item['Goal'] for item in harmful_data]
        else:
            # Fallback: assume it's a flat dataset
            goals = [item['Goal'] for item in dataset]
        
        max_goals = get_config_value(self.config, 'data', 'max_goals', default=None)
        if max_goals:
            goals = goals[:max_goals]
        
        logger.info(f"Processing {len(goals)} goals")
        
        # Process each goal
        for idx, goal_text in enumerate(goals):
            goal_id = f"goal_{idx + 1:03d}"
            logger.info(f"\n{'='*60}")
            logger.info(f"üìé Processing {goal_id}: {goal_text[:80]}...")
            
            result = self._process_goal(goal_id, goal_text)
            self.storage.add_goal_result(result)
            
            logger.info(f"‚úÖ {goal_id} complete: success={result.success}")
        
        # End-of-run tactic generalization
        if self.enable_generalization and self.generalization_module:
            self._perform_end_generalization()
        
        # Save all results
        self.storage.save_all()
        
        # Log final statistics
        self._log_final_stats(goals)
    
    def _process_goal(self, goal_id: str, goal_text: str) -> GoalResult:
        """Process a single goal with retry and learning."""
        trials = []
        knowledge_update = None
        prompt_suggestions = None
        
        for trial_num in range(1, self.max_trials + 1):
            logger.info(f"--- Trial {trial_num}/{self.max_trials} ---")
            
            # Reset attacker for new trial
            self.attacker.reset_for_new_trial()
            
            # Run trial
            trial_result = self._run_trial(
                goal_text, trial_num, prompt_suggestions
            )
            trials.append(trial_result)
            
            if trial_result.success:
                logger.info("üéØ SUCCESS!")
                
                # GALA tactic-wise learning
                if self.enable_gala and self.gala_module:
                    knowledge_update = self.gala_module.perform_tactic_wise_learning(
                        goal_text, trial_result.turns, success=True
                    )
                
                break  # Stop trials on success
            
            else:
                logger.info("‚ùå FAILURE")
                
                # GALA prompt-wise learning for next trial
                if self.enable_gala and self.gala_module and trial_num < self.max_trials:
                    failure_reason = "Target maintained refusal or provided insufficient information"
                    self.gala_module.perform_prompt_wise_learning(
                        goal_text, trial_result.turns, failure_reason
                    )
                    prompt_suggestions = self.gala_module.get_prompt_suggestions(goal_text)
        
        overall_success = any(t.success for t in trials)
        
        return GoalResult(
            goal_id=goal_id,
            goal_text=goal_text,
            success=overall_success,
            trials=trials,
            knowledge_update=knowledge_update
        )
    
    def _run_trial(
        self, 
        goal: str, 
        trial_num: int, 
        prompt_suggestions: Optional[str]
    ) -> TrialResult:
        """Run a single trial for a goal."""
        conversation = ConversationManager()
        target_safety = self.target.get_safety_prompt()
        turns_data = []
        violation_count = 0
        final_pv = 0
        used_traceback = False
        traceback_info = None
        
        for turn_num in range(self.max_turns):
            logger.info(f"  Turn {turn_num + 1}/{self.max_turns}")
            
            # Generate attacker prompt with error handling
            try:
                if turn_num == 0:
                    attacker_prompt = self.attacker.get_initial_plan_gala(
                        goal, target_safety, prompt_suggestions
                    ) if self.enable_gala else self.attacker.get_initial_plan(goal, target_safety)
                else:
                    attacker_prompt = self.attacker.get_followup_plan_gala(
                        goal, target_safety
                    ) if self.enable_gala else self.attacker.get_followup_plan(goal, target_safety)
            except ValueError as e:
                logger.error(f"  Attacker prompt generation failed: {e}")
                logger.info("  Skipping to next trial due to parsing error")
                break  # Break out of turn loop, will try next trial
            
            # Get target response
            target_response = self.target.respond(attacker_prompt)
            
            # Update conversation
            conversation.add_turn(attacker_prompt, target_response)
            
            # Update attacker belief state
            self.attacker.belief_state.add_turn(attacker_prompt, target_response)
            current_tactics = self.attacker.belief_state.strategy_state.get("currentTactic", [])
            self.attacker.belief_state.update_from_response(target_response, current_tactics)
            
            # Judge evaluation
            judge_result = self.judge.evaluate_turn(attacker_prompt, target_response, goal)
            pv = judge_result["violation_intensity"]
            
            # Store turn data
            turn_data = {
                "turn": turn_num + 1,
                "attacker_prompt": attacker_prompt,
                "target_response": target_response,
                "judge_evaluation": {
                    "violation_intensity": pv,
                    "reasoning": judge_result["reasoning"]
                }
            }
            turns_data.append(turn_data)
            
            logger.info(f"    PV={pv}")
            
            # Check for success
            if pv == 2:
                final_pv = 2
                break
            elif pv == 1:
                violation_count += 1
        
        # Determine final status
        if final_pv != 2 and violation_count > 0:
            final_pv = 1
        
        success = final_pv == 2
        reached_max = len(turns_data) >= self.max_turns and not success
        
        # Traceback if enabled and reached max turns without success
        if self.enable_traceback and self.traceback_module and reached_max:
            if self.traceback_module.should_perform_traceback(len(turns_data), True):
                logger.info("  Performing traceback...")
                tb_result = self.traceback_module.perform_traceback(
                    goal, turns_data, target_safety
                )
                used_traceback = True
                traceback_info = tb_result.failure_knowledge
                
                # Store traceback knowledge in attacker
                self.attacker.knowledge_base.failed_attempt_insight[goal] = traceback_info
        
        # Get tactics used
        tactics_used = self.attacker.belief_state.strategy_state.get("tacticsTried", [])
        
        return TrialResult(
            trial_id=trial_num,
            success=success,
            violation_result=final_pv,
            turns=turns_data,
            used_traceback=used_traceback,
            traceback_info=traceback_info,
            prompt_suggestions_used=prompt_suggestions is not None,
            final_belief_state=self.attacker.belief_state.get_state_summary(),
            tactics_used=tactics_used
        )
    
    def _perform_end_generalization(self) -> None:
        """Perform tactic generalization at end of all runs."""
        logger.info("\n" + "="*60)
        logger.info("üìä Performing end-of-run tactic generalization...")
        
        successful_jailbreaks = self.storage.get_successful_jailbreaks()
        
        if not successful_jailbreaks:
            logger.warning("No successful jailbreaks to generalize")
            return
        
        result = self.generalization_module.generalize_all(successful_jailbreaks)
        
        # Save generalization results
        gen_path = get_config_value(self.config, 'output', 'generalized_tactics_path',
                                    default='generalized_tactics.json')
        save_generalization_result(result, gen_path)
        
        logger.info(f"Generalization complete: {len(result.generalized_tactics)} tactics "
                   f"from {result.batches_processed} batches")
    
    def _log_final_stats(self, goals: List[str]) -> None:
        """Log final pipeline statistics."""
        total = len(goals)
        successes = sum(1 for r in self.storage.results.values() if r.success)
        rate = (successes / total * 100) if total > 0 else 0
        
        logger.info("\n" + "="*60)
        logger.info("üéâ PIPELINE COMPLETE!")
        logger.info(f"üìà Success Rate: {rate:.1f}% ({successes}/{total})")
        
        if self.enable_gala and self.gala_module:
            stats = self.gala_module.get_learning_statistics()
            logger.info(f"üß† GALA Stats: {stats['learned_tactics_count']} tactics learned")


def main(config):
    """Main entry point."""
    orchestrator = PipelineOrchestrator(config)
    orchestrator.run()


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--config_path", type=str, required=True,
        help="Path to the pipeline configuration file"
    )
    args = parser.parse_args()
    
    config = OmegaConf.load(args.config_path)
    main(config)
