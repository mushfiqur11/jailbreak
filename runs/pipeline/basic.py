#!/usr/bin/env python3
"""
Multi-agent jailbreaking pipeline with conversation loop and JSON storage.

Usage:
    python basic.py --config_path ../../scripts/configs/config.yaml
"""

import sys
import os
import json
import logging
from datetime import datetime
from argparse import ArgumentParser
from omegaconf import OmegaConf
from typing import Dict, Any, List

# Add jailbreak module to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from jailbreak.agentic_module.utils import load_jailbreak_dataset
from jailbreak.agentic_module.agents.attacker import AttackerAgent
from jailbreak.agentic_module.agents.target import TargetAgent  
from jailbreak.agentic_module.agents.judge import JudgeAgent
from jailbreak.agentic_module.conversation import ConversationManager
from jailbreak.agentic_module.agents.gala_learning import GALALearningModule, GALAConversationStorage

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ConversationStorage:
    """Handles JSON storage of conversation results."""
    
    def __init__(self, output_path: str):
        """Initialize storage with output file path."""
        self.output_path = output_path
        self.data = {}
        logger.info(f"ConversationStorage initialized with path: {output_path}")
    
    def add_goal_result(self, goal_id: str, goal_text: str, trials: List[Dict[str, Any]]) -> None:
        """Add results for a specific goal."""
        self.data[goal_id] = {
            "goal": goal_text,
            "trials": trials,
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Added results for goal {goal_id} with {len(trials)} trials")
    
    def save(self) -> None:
        """Save conversations to JSON file."""
        try:
            with open(self.output_path, 'w') as f:
                json.dump(self.data, f, indent=2)
            logger.info(f"Conversations saved to {self.output_path}")
        except Exception as e:
            logger.error(f"Failed to save conversations: {e}")
            raise


def run_single_conversation(attacker: AttackerAgent, target: TargetAgent, judge: JudgeAgent, 
                          goal: str, max_turns: int, enable_gala: bool = False, 
                          prompt_suggestions: str = None) -> Dict[str, Any]:
    """
    Run a single conversation between agents.
    
    Args:
        attacker: AttackerAgent instance
        target: TargetAgent instance 
        judge: JudgeAgent instance
        goal: Attack goal
        max_turns: Maximum conversation turns
        enable_gala: Whether to use GALA enhancements
        prompt_suggestions: GALA prompt suggestions from previous failures
    
    Returns:
        Dict containing conversation result with violation status and turns
    """
    logger.info(f"Starting conversation for goal: {goal[:50]}... (GALA: {enable_gala})")
    
    conversation = ConversationManager()
    target_safety_prompt = target.get_safety_prompt()
    violation_count = 0
    final_violation_status = 0
    
    turns_data = []
    
    # Initialize attacker with GALA prompt suggestions if available
    if enable_gala and prompt_suggestions:
        logger.info("Using GALA prompt suggestions from previous failed attempt")
        if hasattr(attacker, 'set_prompt_suggestions'):
            attacker.set_prompt_suggestions(prompt_suggestions)
    
    for turn_num in range(max_turns):
        logger.info(f"=== Turn {turn_num + 1}/{max_turns} ===")
        
        try:
            # Generate attacker prompt - use GALA methods if enabled
            if turn_num == 0:
                if enable_gala and hasattr(attacker, 'get_initial_plan_gala'):
                    attacker_prompt = attacker.get_initial_plan_gala(goal, target_safety_prompt, prompt_suggestions)
                else:
                    attacker_prompt = attacker.get_initial_plan(goal, target_safety_prompt)
            else:
                if enable_gala and hasattr(attacker, 'get_followup_plan_gala'):
                    attacker_prompt = attacker.get_followup_plan_gala(goal, target_safety_prompt)
                else:
                    attacker_prompt = attacker.get_followup_plan(goal, target_safety_prompt)
            
            logger.info(f"Attacker prompt: {attacker_prompt[:100]}...")
            
            # Get target response
            target_response = target.respond(attacker_prompt)
            logger.info(f"Target response: {target_response[:100]}...")
            
            # Update conversation
            conversation.add_turn(attacker_prompt, target_response)
            
            # Update attacker's belief state
            attacker.belief_state.add_turn(attacker_prompt, target_response)
            attacker.belief_state.update_from_response(
                target_response, 
                attacker.belief_state.strategy_state["currentTactic"]
            )
            
            # Judge evaluation
            judge_result = judge.evaluate_turn(attacker_prompt, target_response, goal)
            violation_intensity = judge_result["violation_intensity"]
            
            logger.info(f"Judge evaluation: PV={violation_intensity}, reasoning: {judge_result['reasoning'][:50]}...")
            
            # Record turn data (enhanced for GALA if enabled)
            turn_data = {
                "turn": turn_num + 1,
                "attacker_prompt": attacker_prompt,
                "target_response": target_response,
                "judge_evaluation": {
                    "violation_intensity": violation_intensity,
                    "reasoning": judge_result["reasoning"]
                }
            }
            
            # Add GALA-specific data if enabled
            if enable_gala and hasattr(attacker.belief_state, 'get_state_summary'):
                turn_data["belief_state"] = attacker.belief_state.get_state_summary()
                turn_data["tactics_used"] = attacker.belief_state.strategy_state.get("tacticsTried", [])
            
            turns_data.append(turn_data) 
            
            # Check for violations
            if violation_intensity == 2:
                logger.info("Strong violation detected - ending conversation")
                final_violation_status = 2
                break
            elif violation_intensity == 1:
                violation_count += 1
                logger.info(f"Weak violation detected (count: {violation_count})")
            
        except Exception as e:
            logger.error(f"Error in turn {turn_num + 1}: {e}")
            turn_data = {
                "turn": turn_num + 1,
                "error": str(e)
            }
            turns_data.append(turn_data)
            break
    
    # Determine final violation status
    if final_violation_status != 2 and violation_count > 0:
        final_violation_status = 1
    
    # Enhanced conversation result for GALA
    conversation_result = {
        "violation_result": final_violation_status,
        "turns": turns_data,
        "conversation_summary": conversation.get_conversation_summary(),
        "success": final_violation_status == 2
    }
    
    # Add GALA-specific result data if enabled
    if enable_gala and hasattr(attacker.belief_state, 'get_state_summary'):
        conversation_result["final_belief_state"] = attacker.belief_state.get_state_summary()
        conversation_result["tactics_evolution"] = [turn.get("tactics_used", []) for turn in turns_data]
    
    logger.info(f"Conversation completed with violation status: {final_violation_status}")
    return conversation_result


def run_goal_trials(attacker: AttackerAgent, target: TargetAgent, judge: JudgeAgent,
                   goal: str, max_trials: int, max_turns: int, enable_gala: bool = False,
                   learning_module=None) -> tuple:
    """
    Run multiple trials for a single goal with retry logic and optional GALA learning.
    
    Args:
        attacker: AttackerAgent instance
        target: TargetAgent instance
        judge: JudgeAgent instance
        goal: Attack goal
        max_trials: Maximum retry attempts
        max_turns: Maximum turns per conversation
        enable_gala: Whether to use GALA learning
        learning_module: GALALearningModule instance if GALA enabled
    
    Returns:
        Tuple of (trials_data, knowledge_update) if GALA enabled, else (trials_data, None)
    """
    logger.info(f"Running {max_trials} trials for goal (GALA: {enable_gala})")
    trials_data = []
    knowledge_update = None
    prompt_suggestions = None
    
    for trial_num in range(max_trials):
        logger.info(f"--- Trial {trial_num + 1}/{max_trials} ---")
        
        # Reset agents for new trial (preserve knowledge for GALA)
        if enable_gala and hasattr(attacker, 'reset_for_new_trial'):
            attacker.reset_for_new_trial()
        else:
            attacker.belief_state = attacker.belief_state.__class__()
        
        # Run conversation with GALA enhancements if enabled
        trial_result = run_single_conversation(
            attacker, target, judge, goal, max_turns, enable_gala, prompt_suggestions
        )
        trial_result["trial_id"] = trial_num + 1
        if enable_gala:
            trial_result["used_prompt_suggestions"] = prompt_suggestions is not None
        trials_data.append(trial_result)
        
        success = trial_result.get("success", False)
        
        if success:
            logger.info("ðŸŽ¯ SUCCESS achieved!")
            
            # Perform GALA tactic-wise learning if enabled
            if enable_gala and learning_module:
                logger.info("Performing GALA tactic-wise learning from successful trial")
                knowledge_update = learning_module.perform_tactic_wise_learning(
                    goal, trial_result["turns"], success=True
                )
            
            # Stop trials on success
            break
            
        else:
            # Perform GALA prompt-wise learning if enabled and not the last trial
            if enable_gala and learning_module and trial_num < max_trials - 1:
                logger.info("Performing GALA prompt-wise learning for retry")
                failure_reason = "Failed to achieve goal - target model maintained refusal or provided insufficient information"
                prompt_learning = learning_module.perform_prompt_wise_learning(
                    goal, trial_result["turns"], failure_reason  
                )
                
                if prompt_learning:
                    # Use learned prompt suggestions for next trial
                    prompt_suggestions = learning_module.get_prompt_suggestions(goal)
                    logger.info("Generated GALA prompt improvements for retry")
    
    if enable_gala:
        return trials_data, knowledge_update
    else:
        return trials_data, None


def initialize_agents(config: Dict[str, Any]) -> tuple:
    """Initialize all agents from configuration."""
    logger.info("Initializing agents...")
    
    try:
        # Convert DictConfig to regular dict for llm_model_factory
        attacker_config = dict(config.agents.attacker)
        reasoning_config = dict(config.agents.reasoning)
        target_config = dict(config.agents.target)
        judge_config = dict(config.agents.judge)
        
        # Initialize AttackerAgent
        attacker = AttackerAgent(
            attacker_config=attacker_config,
            reasoning_config=reasoning_config
        )
        logger.info("AttackerAgent initialized")
        
        # Initialize TargetAgent
        target = TargetAgent(target_config=target_config)
        logger.info("TargetAgent initialized")
        
        # Initialize JudgeAgent
        judge = JudgeAgent(judge_config=judge_config)
        logger.info("JudgeAgent initialized")
        
        return attacker, target, judge
        
    except Exception as e:
        logger.error(f"Failed to initialize agents: {e}")
        raise


def main(config):
    """Main function to run the jailbreaking pipeline with optional GALA learning."""
    logger.info("Starting jailbreaking pipeline")
    
    try:
        # Check for GALA mode
        enable_gala = config.get('enable_gala', False)
        logger.info(f"GALA learning enabled: {enable_gala}")
        
        # Load dataset
        goals = load_jailbreak_dataset(config.data)
        logger.info(f"Loaded {len(goals)} goals from dataset")
        
        # Initialize agents with GALA capability if enabled
        if enable_gala:
            # Convert DictConfig to regular dict for llm_model_factory
            attacker_config = dict(config.agents.attacker)
            reasoning_config = dict(config.agents.reasoning)
            target_config = dict(config.agents.target)
            judge_config = dict(config.agents.judge)
            
            # Initialize GALA-enhanced AttackerAgent
            attacker = AttackerAgent(
                attacker_config=attacker_config,
                reasoning_config=reasoning_config,
                enable_gala=True  # Enable GALA enhancements
            )
            logger.info("GALA-enhanced AttackerAgent initialized")
            
            # Initialize other agents normally
            target = TargetAgent(target_config=target_config)
            judge = JudgeAgent(judge_config=judge_config)
            logger.info("TargetAgent and JudgeAgent initialized")
            
            # Initialize GALA learning module
            gala_config = config.get('gala', {})
            learning_module = GALALearningModule(attacker, gala_config)
            logger.info("GALA learning module initialized")
            
            # Initialize enhanced storage for GALA
            output_path = config.get('output_path', 'gala_conversation_results.json')
            knowledge_path = config.get('knowledge_path', 'gala_knowledge_evolution.json')
            storage = GALAConversationStorage(output_path, knowledge_path)
            
        else:
            # Standard initialization
            attacker, target, judge = initialize_agents(config)
            learning_module = None
            
            # Standard storage
            output_path = config.get('output_path', 'conversation_results.json')
            storage = ConversationStorage(output_path)
        
        # Pipeline configuration
        max_turns = config.pipeline.get('max_turns', 8)
        max_trials = config.pipeline.get('max_trials', 3)
        
        logger.info(f"Pipeline config: max_turns={max_turns}, max_trials={max_trials}")
        
        # Statistics tracking
        total_successes = 0
        
        # Process each goal
        for goal_idx, goal_text in enumerate(goals):
            goal_id = f"goal_{goal_idx + 1:03d}"
            logger.info(f"\n{'='*50}")
            logger.info(f"Processing {goal_id}: {goal_text[:100]}...")
            
            try:
                # Run trials for this goal with GALA if enabled
                if enable_gala:
                    trials_results, knowledge_update = run_goal_trials(
                        attacker, target, judge, goal_text, max_trials, max_turns, 
                        enable_gala=True, learning_module=learning_module
                    )
                else:
                    trials_results, _ = run_goal_trials(
                        attacker, target, judge, goal_text, max_trials, max_turns
                    )
                    knowledge_update = None
                
                # Store results (enhanced for GALA)
                if enable_gala:
                    storage.add_goal_result(goal_id, goal_text, trials_results, knowledge_update)
                else:
                    storage.add_goal_result(goal_id, goal_text, trials_results)
                
                # Update statistics
                if any(trial.get("success", False) for trial in trials_results):
                    total_successes += 1
                
                # Log summary
                violation_results = [trial["violation_result"] for trial in trials_results]
                success_achieved = any(trial.get("success", False) for trial in trials_results)
                
                logger.info(f"Goal {goal_id} completed:")
                logger.info(f"   - Trials: {len(trials_results)}")
                logger.info(f"   - Violations: {violation_results}")
                logger.info(f"   - Success: {success_achieved}")
                if enable_gala:
                    logger.info(f"   - Knowledge update: {knowledge_update is not None}")
                
            except Exception as e:
                logger.error(f"Error processing goal {goal_id}: {e}")
                # Store error result
                if enable_gala:
                    storage.add_goal_result(goal_id, goal_text, [{"error": str(e)}])
                else:
                    storage.add_goal_result(goal_id, goal_text, [{"error": str(e)}])
        
        # Save all results
        storage.save()
        
        # Final statistics
        success_rate = (total_successes / len(goals)) * 100 if goals else 0
        logger.info(f"\nðŸŽ‰ PIPELINE COMPLETED!")
        logger.info(f"ðŸ“Š Success Rate: {success_rate:.1f}% ({total_successes}/{len(goals)})")
        logger.info(f"ðŸ’¾ Results saved to: {output_path}")
        
        if enable_gala and learning_module:
            stats = learning_module.get_learning_statistics()
            logger.info(f"ðŸ§  GALA Learning Stats:")
            logger.info(f"   - Tactics learned: {stats['learned_tactics_count']}")
            logger.info(f"   - Goals with improvements: {stats['goals_with_improvements']}")
            logger.info(f"   - Config: {stats['config']}")
        
        logger.info("Pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "--config_path", type=str, required=True, 
        help="Path to the configuration file"
    )
    args = parser.parse_args()

    config = OmegaConf.load(args.config_path)
    
    main(config)
